{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prerequisite**\n",
    "- Nerual networks\n",
    "\n",
    "**keras**:\n",
    "- keras is higher level api\n",
    "- when building nn we will use keras and depending on the flexibility that we need when creating model we will use different api of kearas. In this section we will use **sequential** and **functional** api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TFF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "## for initializing gpu we can comment out the below two lines of code\n",
    "# physical_devices=tf.config.list_physical_device(\"GPU\")\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training dataset has 60k gray scale images of shape 28*28. and 10k images in the test set. We need to reshape each image pixels into single column in order to train. We will also normalize images into range [0 to 1] from [0-255] so that model will learn fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape(-1,28*28).astype(\"float32\")/255.0 \n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also need to convert the image vectors into tensors by following command.\n",
    "x_train=tf.convert_to_tensor(x_train). but this is done internally by tensorflow. so if it's numpy arrays we don't need to be bother much. the conversion is happen automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **sequential model**:\n",
    "Now, I will build neural networks using sequential api of keras. It is convenient but not very flexible. It's only allow one input map to one output. It is a major limitation. that's why we need sequential of such input output mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sequential model\n",
    "sequential_model=keras.Sequential(\n",
    "[keras.Input(shape=(28*28)), #this input is for see the summary\n",
    "layers.Dense(512,activation='relu'),\n",
    " layers.Dense(256,activation='relu'),\n",
    " layers.Dense(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, I initialized neural network model using sequential model. The process is very simple. Just passed a list of layers. Here initialized two hidden lalyers having neurons 512,256 respectively and declared relu activations for these two layers. Lastly an output layer having 10 neurons that is the number of class labels. We did't declare activation (softmax) in the output layer. we will use it inside the loss function.\n",
    "\n",
    "We use Input to see the model summary. We can execute the summary as follows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have the  model declaration in other way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=keras.Sequential()\n",
    "# model.add(keras.Input(shape=(784)))\n",
    "# print(model.summary())\n",
    "# model.add(layers.Dense(512,activation='relu'))\n",
    "# print(model.summary())\n",
    "# model.add(layers.Dense(256,activation='relu'))\n",
    "# print(model.summary())\n",
    "# model.add(layers.Dense(10))\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to specify layer's output\n",
    "# model=keras.Model(inputs=model.inputs,outputs=[model.layers[-1].output])\n",
    "# features=model.predict(x_train)\n",
    "# print(feature.shape )\n",
    "# #to print all outputs of each layer we can use loop\n",
    "# for feature in features:\n",
    "#     print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these declaration method we have the opportunity to see model summary after each layer. It's a effective debugging tool for larger neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 535818 (2.04 MB)\n",
      "Trainable params: 535818 (2.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sequential_model.summary())\n",
    "# import sys\n",
    "# sys.exit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also see model summary without taking keras.Input() in model list. In that case we have to print model summary after model.fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, going to tell keras how to configure our training part of our network. For example, we are going to specify the loss function we want to use. we are doing this by using model.compile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "sequential_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(lr=.001),\n",
    "    metrics=[\"accuracy\"],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in above, \n",
    "- \"Sparses\" means the labels are just itegers for corresponding labels.if we remove this Sparse then we would need to have one hot encoding.\n",
    "- The argument \"from_logits=True\" this is because we don't have a softmax activation in the model. So when we set from_logits=True it's going  to send it to a softmax first. Then it's going to map sparse categorical cross entropy loss.\n",
    "- metrics keeps tracks the running acc so far.\n",
    "\n",
    "In conclusion, model.compile specifies configuration of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 - 17s - loss: 0.1827 - accuracy: 0.9444 - 17s/epoch - 9ms/step\n",
      "Epoch 2/5\n",
      "1875/1875 - 15s - loss: 0.0795 - accuracy: 0.9748 - 15s/epoch - 8ms/step\n",
      "Epoch 3/5\n",
      "1875/1875 - 17s - loss: 0.0532 - accuracy: 0.9833 - 17s/epoch - 9ms/step\n",
      "Epoch 4/5\n",
      "1875/1875 - 16s - loss: 0.0415 - accuracy: 0.9871 - 16s/epoch - 9ms/step\n",
      "Epoch 5/5\n",
      "1875/1875 - 16s - loss: 0.0324 - accuracy: 0.9895 - 16s/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dfaff65f10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_model.fit(x_train,y_train,batch_size=32,epochs=5,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use model.fit to specify more concrete training of thee network.\n",
    "- verbose=2 so that prints after each epoch. \n",
    "\n",
    "Now after training we want to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0869 - accuracy: 0.9785 - 810ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08692192286252975, 0.9785000085830688]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_model.evaluate(x_test,y_test,batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we don't need to set epoch. Because we want to train for 1 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, The sequential api is convenient to use. In situation where it can't be used we can use functional api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "- more flexible\n",
    "- can handle multiple input and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional model\n",
    "inputs=keras.Input(shape=(784))\n",
    "x=layers.Dense(512,activation='relu',name=\"first_layer\")(inputs)\n",
    "x=layers.Dense(256,activation='relu',name=\"second_layer\")(x)\n",
    "outputs=layers.Dense(10,activation='softmax',name=\"output_layer\")(x)\n",
    "\n",
    "functional_model=keras.Model(inputs=inputs,outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the model will take inputs and outputs that we declared as inputs and outputs of the network and will build the model.\n",
    "- unlike before, we are now using softmax in the output layer. so we need to change configuration of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " first_layer (Dense)         (None, 512)               401920    \n",
      "                                                                 \n",
      " second_layer (Dense)        (None, 256)               131328    \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 535818 (2.04 MB)\n",
      "Trainable params: 535818 (2.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(functional_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "functional_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), #by default false\n",
    "    optimizer=keras.optimizers.Adam(lr=.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 - 17s - loss: 0.1853 - accuracy: 0.9440 - 17s/epoch - 9ms/step\n",
      "Epoch 2/5\n",
      "1875/1875 - 16s - loss: 0.0791 - accuracy: 0.9752 - 16s/epoch - 8ms/step\n",
      "Epoch 3/5\n",
      "1875/1875 - 19s - loss: 0.0560 - accuracy: 0.9820 - 19s/epoch - 10ms/step\n",
      "Epoch 4/5\n",
      "1875/1875 - 17s - loss: 0.0413 - accuracy: 0.9869 - 17s/epoch - 9ms/step\n",
      "Epoch 5/5\n",
      "1875/1875 - 18s - loss: 0.0326 - accuracy: 0.9892 - 18s/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dfb1751010>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "functional_model.fit(x_train,y_train,batch_size=32,epochs=5,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0708 - accuracy: 0.9797 - 789ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07081051915884018, 0.9797000288963318]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "functional_model.evaluate(x_test,y_test,batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUGGESTION FOR **FUNCTIONAL AND SEQUENTIAL API**\n",
    "\n",
    "- using different layer size(increasing particularly), increasing training time it is possible to get over 98.2% on the test set!\n",
    "-  try different optimization other than Adam. e.g. Gradient Descent with momentum, Adagrad, and RMSprop\n",
    "- Check out with and without normalization and see the differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
