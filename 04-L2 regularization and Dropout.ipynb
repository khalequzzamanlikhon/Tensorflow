{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw overfitting which means the gap between trainig accuracy and test accuracy was high. to overcome overfitting we can use different regularization methods and dropout. In this example I will use L2 regularization and dropout. **Dropout** is a way of disconnecting some connections between neurons of two consecutive layers in order to increase CNN architecture. Moreover, we used batch normalization for faster training which also plays role as regularizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TFF_CPP_MIN_LOG_LEVEL\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3,padding=\"same\",kernel_regularizer=regularizers.l2(0.01),)(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.activations.relu(x)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "\n",
    "x = layers.Conv2D(64, 3,padding=\"same\",kernel_regularizer=regularizers.l2(0.01),)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.activations.relu(x)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "\n",
    "x = layers.Conv2D(128, 3,padding=\"same\",kernel_regularizer=regularizers.l2(.01))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = keras.activations.relu(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\",kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x=layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,batch_size=64,epochs=150,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation \n",
    "model.evaluate(x_test,y_test,batch_size=64,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see here that the gap between the training accuracy and the test accuracy has decreased using regularization and dropout. Althouth the train accuracy can be higher by increasing the training time(increasing the batch size)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
