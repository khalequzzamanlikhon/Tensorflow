{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will discuss about text classification (binary) on \"imdb revies\" from tensorflow dataset. The workflow of this taks is as follows:\n",
    "\n",
    "- Loading the dataset\n",
    "- Text preprocessing\n",
    "- Model acrchitecture\n",
    "- Training\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the dataset** \n",
    "\n",
    "In this section I will load data from tensorflow datasets. This is a text data for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.13.0\n",
      "tf dataset version 4.9.2\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version\",tf.__version__)\n",
    "print(\"tf dataset version\",tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True, # will return tuple (text, label) otherwise dict\n",
    "    with_info=True,  #able to get info about dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
      "    classification containing substantially more data than previous benchmark\n",
      "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
      "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='C:\\\\Users\\\\klikh\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# info of the dataset\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# let's see only one exaplme\n",
    "for text, label in ds_train:\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Data**\n",
    "\n",
    "We cannot send a entire text to our model. We need to tokenize it. After tokenaization the sentence will return a list of words. For example: \"I love the movie\"--> TOKENIZATION-->[\"I\",\"love\",\"the\",\"movie\"]. Then we cannot send the text in the model, we need to numericalize. So, to make the compatible input for the ml model the text data need two transfomation.\n",
    "- tokenize the text\n",
    "- numericalize the tokenized text\n",
    "\n",
    "\n",
    "\n",
    "There are many built in functions available in tensorflow. Here I will use method from tensorflow datasets. One thing that, here I have used tfds.deprecated.text.Tokenizer() instead of tfds.features.text.Tokenizer(). Use one that runs without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**build vocabulary**: taking only the unique words I will build vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(min_appearance):\n",
    "    word_counts = {}\n",
    "    for text, _ in ds_train:\n",
    "        tokens = tokenizer.tokenize(text.numpy().lower()) #list\n",
    "        for token in tokens:\n",
    "            word_counts[token] = word_counts.get(token, 0) + 1\n",
    "    \n",
    "    vocabulary = {word for word, count in word_counts.items() if count >= min_appearance}\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "min_appearance=2\n",
    "vocabulary = build_vocabulary(min_appearance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numericalize all of the tokenized words**: the encoder function below will convert the text in the vocabulary to numerical sequences. The vocabulary we created is a set. But we need to convert it into list in order to make it as an input to the tokenTextEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.deprecated.text.TokenTextEncoder(\n",
    "    list(vocabulary), oov_token=\"<UNK>\", lowercase=True, tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_enc(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function \"encode_map\" is used to use python function(here my_enc) to tensorflow tensors. Moreover, the tensorflow datasets work efficiently if the shape is determined explicitly.. Here the shape is set to \"None\" because of the variable length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_map_fn(text, label):\n",
    "    # py_func doesn't set the shape of the returned tensors.\n",
    "    encoded_text, label = tf.py_function(\n",
    "        my_enc, inp=[text, label], Tout=(tf.int64, tf.int64)\n",
    "    )\n",
    "    # setting the shape of the tensors to None for variable length sequence\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = ds_train.map(encode_map_fn, num_parallel_calls=AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(1000)\n",
    "ds_train = ds_train.padded_batch(batch_size, padded_shapes=([None], ()))\n",
    "ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(encode_map_fn)\n",
    "ds_test = ds_test.padded_batch(batch_size, padded_shapes=([None], ()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing part is complete. We have the encoded text tensors with corresponding label. Now I will define a simple sequential model using keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Masking(mask_value=0),\n",
    "        layers.Embedding(input_dim=len(vocabulary) + 2, output_dim=32),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(16, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.Dropout(.5),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(3e-4, clipnorm=1),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 31s - loss: 0.7007 - accuracy: 0.5000 - 31s/epoch - 40ms/step\n",
      "Epoch 2/20\n",
      "782/782 - 26s - loss: 0.6692 - accuracy: 0.5044 - 26s/epoch - 33ms/step\n",
      "Epoch 3/20\n",
      "782/782 - 26s - loss: 0.5917 - accuracy: 0.6219 - 26s/epoch - 33ms/step\n",
      "Epoch 4/20\n",
      "782/782 - 25s - loss: 0.5141 - accuracy: 0.7508 - 25s/epoch - 33ms/step\n",
      "Epoch 5/20\n",
      "782/782 - 25s - loss: 0.4662 - accuracy: 0.8017 - 25s/epoch - 32ms/step\n",
      "Epoch 6/20\n",
      "782/782 - 25s - loss: 0.4300 - accuracy: 0.8337 - 25s/epoch - 32ms/step\n",
      "Epoch 7/20\n",
      "782/782 - 25s - loss: 0.4054 - accuracy: 0.8485 - 25s/epoch - 32ms/step\n",
      "Epoch 8/20\n",
      "782/782 - 25s - loss: 0.3848 - accuracy: 0.8609 - 25s/epoch - 32ms/step\n",
      "Epoch 9/20\n",
      "782/782 - 26s - loss: 0.3682 - accuracy: 0.8682 - 26s/epoch - 33ms/step\n",
      "Epoch 10/20\n",
      "782/782 - 25s - loss: 0.3527 - accuracy: 0.8773 - 25s/epoch - 33ms/step\n",
      "Epoch 11/20\n",
      "782/782 - 26s - loss: 0.3372 - accuracy: 0.8843 - 26s/epoch - 33ms/step\n",
      "Epoch 12/20\n",
      "782/782 - 27s - loss: 0.3254 - accuracy: 0.8904 - 27s/epoch - 34ms/step\n",
      "Epoch 13/20\n",
      "782/782 - 28s - loss: 0.3096 - accuracy: 0.8977 - 28s/epoch - 36ms/step\n",
      "Epoch 14/20\n",
      "782/782 - 30s - loss: 0.2996 - accuracy: 0.9021 - 30s/epoch - 38ms/step\n",
      "Epoch 15/20\n",
      "782/782 - 29s - loss: 0.2905 - accuracy: 0.9060 - 29s/epoch - 37ms/step\n",
      "Epoch 16/20\n",
      "782/782 - 30s - loss: 0.2805 - accuracy: 0.9096 - 30s/epoch - 38ms/step\n",
      "Epoch 17/20\n",
      "782/782 - 28s - loss: 0.2707 - accuracy: 0.9141 - 28s/epoch - 36ms/step\n",
      "Epoch 18/20\n",
      "782/782 - 27s - loss: 0.2611 - accuracy: 0.9187 - 27s/epoch - 35ms/step\n",
      "Epoch 19/20\n",
      "782/782 - 26s - loss: 0.2574 - accuracy: 0.9208 - 26s/epoch - 33ms/step\n",
      "Epoch 20/20\n",
      "782/782 - 26s - loss: 0.2464 - accuracy: 0.9247 - 26s/epoch - 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x109c1aaaa90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=20, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 14s - loss: 0.3291 - accuracy: 0.8822 - 14s/epoch - 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3291319012641907, 0.8822399973869324]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate on the test set\n",
    "model.evaluate(ds_test,verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
